---
layout: post
title: 特征重要性评估(Feature Importance Evaluation) 
tags: [lua文章]
categories: [topic]
---
<p>特征对目标变量预测的相对重要性，可以通过决策树中使用特征作为决策节点的相对顺序来评估。<br/>决策树顶部使用的特征，将对更多样本的最终预测决策做出贡献。<br/>因此，可以通过每个特征对最终预测做出贡献的样本比例，来评估该特征的重要性。</p>
<p>通过对多个随机树中的预期贡献率取平均，可以减少这种估计的方差。</p>
<p>sklearn.ensemble模块包含两个基于随机决策树的平均算法：Random Forest 和 Extra-Tress.<br/>这两种算法都是在构造过程中引入随机性来创建一组不同的分类器。</p>
<h4 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h4><p>实例一: 特征重要性二维可视化<br/><img src="https://mirokule.github.io//img/featureImportance_1.png" width="500"/><br/></p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br/><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br/><span class="line"></span><br/><span class="line"><span class="keyword">from</span> sklearn.dataset <span class="keyword">import</span> fetch_olivetti_faces</span><br/><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> ExtraTreesClassifier</span><br/><span class="line"></span><br/><span class="line">n_jobs=<span class="number">1</span> </span><br/><span class="line"></span><br/><span class="line"><span class="comment"># 加载数据</span></span><br/><span class="line">data = fetch_oliveetti_faces()</span><br/><span class="line">X = data.images.reshape((len(data.images),<span class="number">-1</span>))</span><br/><span class="line">y = data.target</span><br/><span class="line"></span><br/><span class="line"><span class="comment"># 构建学习器</span></span><br/><span class="line">forest = ExtraTreesClassifier(n_estimators=<span class="number">1000</span>, max_features=<span class="number">128</span>, n_jobs=n_jobs)</span><br/><span class="line">forest.fit(X,y)</span><br/><span class="line"></span><br/><span class="line"><span class="comment"># 输出特征重要性</span></span><br/><span class="line">importances = forest.feature_importances_</span><br/><span class="line">importances = importances.reshape(data.image[<span class="number">0</span>].shape)</span><br/><span class="line"></span><br/><span class="line"><span class="comment"># 绘图</span></span><br/><span class="line">plt.matshow(importances. cmap=plt.cm.hot)</span><br/><span class="line">plt.title(<span class="string">&#34;Pixel importances with forests of trees&#34;</span>)</span><br/><span class="line">plt.show()</span><br/></pre></td></tr></tbody></table></figure><p></p>
<p>实例二：特征重要性一维可视化<br/><img src="https://mirokule.github.io//img/featureImportance_2.png" width="600"/><br/></p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br/><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br/><span class="line"></span><br/><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br/><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> ExtraTreesClassifier</span><br/><span class="line"></span><br/><span class="line"><span class="comment"># 生成测试数据</span></span><br/><span class="line">X,y = make_classification(n_samples=<span class="number">1000</span>, n_features=<span class="number">10</span>, n_informative=<span class="number">3</span>, n_classes=<span class="number">2</span>)</span><br/><span class="line"></span><br/><span class="line"><span class="comment"># 选取学习器</span></span><br/><span class="line">forest = ExtraTreesClassifier(n_estimators=<span class="number">250</span>, random_state=<span class="number">0</span>)</span><br/><span class="line">forest.fit(X,y)</span><br/><span class="line"></span><br/><span class="line"><span class="comment"># 输出特征重要性</span></span><br/><span class="line">importances = forest.feature_importances_</span><br/><span class="line">std = np.std([tree.feature_importances_ <span class="keyword">for</span> tree <span class="keyword">in</span> forest.estimators_], axis=<span class="number">0</span>)</span><br/><span class="line">indices = np.argsort(importances)[::<span class="number">-1</span>]</span><br/><span class="line"></span><br/><span class="line"><span class="comment"># 打印</span></span><br/><span class="line">print(<span class="string">&#34;Feature ranking:&#34;</span>)</span><br/><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br/><span class="line">    print(<span class="string">&#34;%d. feature %d (%f)&#34;</span> % (f+<span class="number">1</span>, indices[f], importances[indices[f]]))</span><br/><span class="line"></span><br/><span class="line"><span class="comment"># 绘图</span></span><br/><span class="line">plt.figure()</span><br/><span class="line">plt.title(<span class="string">&#34;Feature importances&#34;</span>)</span><br/><span class="line">plt.bar(range(X.shape[<span class="number">1</span>]), importances[indices], color=<span class="string">&#39;r&#39;</span>, yerr=std[indices], align=<span class="string">&#39;center&#39;</span>)</span><br/><span class="line">plt.xticks(range(X.shape[<span class="number">1</span>]), indices)</span><br/><span class="line">plt.xlim([<span class="number">-1</span>, X.shape[<span class="number">1</span>]])</span><br/><span class="line">plt.show()</span><br/></pre></td></tr></tbody></table></figure><p></p>