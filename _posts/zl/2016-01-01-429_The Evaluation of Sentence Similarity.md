---
layout: post
title: The Evaluation of Sentence Similarity 
tags: [lua文章]
categories: [topic]
---
<p>I am trying to write my first english blog based on two reasons: First, the data set used in this blog is english; Second, I’d like to expand my reach and attract more audiences, although I should admit that nobody cares.</p>
<h2 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h2><p>Initially I want to use chinese corpus, but I cannot find a proper one. The data should sound like this one:</p>
<blockquote>
<p>word1    word2    similarity score<br/>阿拉伯人    阿拉伯    7.2<br/>畜产    农业    5.6<br/>垂涎    崇敬    3.4<br/>次序    秩序    4.7<br/>定心丸    药品    4.3<br/>房租    价格    5.2<br/>翡翠    宝石    6.7<br/>高科技    技术    7.5<br/>购入    购买    8.5<br/>观音    菩萨    8.2<br/>归并    合并    7.7</p>
</blockquote>

<p>not like this:</p>
<blockquote>
<p>为何我无法申请开通花呗信用卡收款    支付宝开通信用卡花呗收款不符合条件怎么回事    1<br/>花呗分期付款会影响使用吗    花呗分期有什么影响吗    0<br/>为什么我花呗没有临时额度    花呗没有临时额度怎么可以负    0<br/>能不能开花呗老兄    花呗逾期了还能开通    0<br/>我的怎么开通花呗收钱    这个花呗是个什么啥？我没开通 我怎么有账单    0<br/>蚂蚁借呗可以停掉么    蚂蚁借呗为什么给我关掉了    0<br/>我想把花呗功能关了    我去饭店吃饭，能用花呗支付吗    0<br/>为什么我借呗开通了又关闭了    为什么借呗存在风险    0<br/>支付宝被冻了花呗要怎么还    支付功能冻结了，花呗还不了怎么办    1</p>
</blockquote>
<p>If you can find the dataset where ‘similarity score’ is double, please donot hesitate to <a href="mailto:jiajizhengbuaa@gmail.com" target="_blank" rel="noopener noreferrer">email me.</a></p>
<p>So, the choice has to be enlgish corpus. The dataset used in this experiment are <a href="http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark" target="_blank" rel="noopener noreferrer">STSbenchmark</a> and SICK data. The SICK data contains 10,000 sentence paris labeled with semantic relatedness and entailment relation.<br/><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1sqxx3vl0j219e07yaam.jpg" alt=""/></p>
<h2 id="Similarity-Methods"><a href="#Similarity-Methods" class="headerlink" title="Similarity Methods"></a>Similarity Methods</h2><h3 id="Baseline"><a href="#Baseline" class="headerlink" title="Baseline"></a>Baseline</h3><p>As the baseline, we just take the embedding of the words in sentence, and compute the average, weighted by frequency of each word.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/><span class="line">12</span><br/><span class="line">13</span><br/><span class="line">14</span><br/><span class="line">15</span><br/><span class="line">16</span><br/><span class="line">17</span><br/><span class="line">18</span><br/><span class="line">19</span><br/><span class="line">20</span><br/><span class="line">21</span><br/><span class="line">22</span><br/><span class="line">23</span><br/><span class="line">24</span><br/><span class="line">25</span><br/><span class="line">26</span><br/><span class="line">27</span><br/><span class="line">28</span><br/><span class="line">29</span><br/><span class="line">30</span><br/><span class="line">31</span><br/><span class="line">32</span><br/></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="params">(sentences1, sentences2, model=None, use_stoplist=False, doc_freqs=None)</span>:</span></span><br/><span class="line">    <span class="keyword">if</span> doc_freqs <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br/><span class="line">        N = doc_freqs[<span class="string">&#34;NUM_DOCS&#34;</span>]</span><br/><span class="line"></span><br/><span class="line">    sims = []</span><br/><span class="line">    <span class="keyword">for</span> (sent1, sent2) <span class="keyword">in</span> zip(sentences1, sentences2):</span><br/><span class="line"></span><br/><span class="line">        tokens1 = sent1.tokens_without_stop <span class="keyword">if</span> use_stoplist <span class="keyword">else</span> sent1.tokens</span><br/><span class="line">        tokens2 = sent2.tokens_without_stop <span class="keyword">if</span> use_stoplist <span class="keyword">else</span> sent2.tokens</span><br/><span class="line"></span><br/><span class="line">        tokens1 = [token <span class="keyword">for</span> token <span class="keyword">in</span> tokens1 <span class="keyword">if</span> token <span class="keyword">in</span> model]</span><br/><span class="line">        tokens2 = [token <span class="keyword">for</span> token <span class="keyword">in</span> tokens2 <span class="keyword">if</span> token <span class="keyword">in</span> model]</span><br/><span class="line">l</span><br/><span class="line">        <span class="keyword">if</span> len(tokens1) == <span class="number">0</span> <span class="keyword">or</span> len(tokens2) == <span class="number">0</span>:</span><br/><span class="line">            sims.append(<span class="number">0</span>)</span><br/><span class="line">            <span class="keyword">continue</span></span><br/><span class="line"></span><br/><span class="line">        tokfreqs1 = Counter(tokens1)</span><br/><span class="line">        tokfreqs2 = Counter(tokens2)</span><br/><span class="line"></span><br/><span class="line">        weights1 = [tokfreqs1[token] * math.log(N / (doc_freqs.get(token, <span class="number">0</span>) + <span class="number">1</span>))</span><br/><span class="line">                    <span class="keyword">for</span> token <span class="keyword">in</span> tokfreqs1] <span class="keyword">if</span> doc_freqs <span class="keyword">else</span> <span class="keyword">None</span></span><br/><span class="line">        weights2 = [tokfreqs2[token] * math.log(N / (doc_freqs.get(token, <span class="number">0</span>) + <span class="number">1</span>))</span><br/><span class="line">                    <span class="keyword">for</span> token <span class="keyword">in</span> tokfreqs2] <span class="keyword">if</span> doc_freqs <span class="keyword">else</span> <span class="keyword">None</span></span><br/><span class="line"></span><br/><span class="line">        embedding1 = np.average([model[token] <span class="keyword">for</span> token <span class="keyword">in</span> tokfreqs1], axis=<span class="number">0</span>, weights=weights1).reshape(<span class="number">1</span>, <span class="number">-1</span>)</span><br/><span class="line">        embedding2 = np.average([model[token] <span class="keyword">for</span> token <span class="keyword">in</span> tokfreqs2], axis=<span class="number">0</span>, weights=weights2).reshape(<span class="number">1</span>, <span class="number">-1</span>)</span><br/><span class="line"></span><br/><span class="line">        sim = cosine_similarity(embedding1, embedding2)[<span class="number">0</span>][<span class="number">0</span>]</span><br/><span class="line">        sims.append(sim)</span><br/><span class="line"></span><br/><span class="line">    <span class="keyword">return</span> sims</span><br/></pre></td></tr></tbody></table></figure>
<h3 id="Smooth-Inverse-Frequency"><a href="#Smooth-Inverse-Frequency" class="headerlink" title="Smooth Inverse Frequency"></a>Smooth Inverse Frequency</h3><p>The baseline, like we did before, is very simple and crude of computing sentence embedding. Word frequency cannot reliably reflect its importance to sentence, semantically speaking. Smooth Inverse Frequency (SIF) tries to solve this problem.</p>
<ul>
<li>SIF is very similar to the weighted average we used before, with the difference that it’s weighted by this formular.<br/>$$<br/>operatorname { SIF } ( w ) = frac { a } { ( a + p ( w ) )}<br/>$$<br/>where $a$ is a hyper-parameter (set to 0.001 by default) and $ p(w)$ is the estimated word frequency in the corpus. (这个权重和 TF或者 IDF 都是不相同的)</li>
<li>we need to perform common component removal: subtract from the sentence embedding obtained above the first principal component of the matrix. This corrects for the influence of high-frequency words that have syntactic or dicourse function, such as ‘but’, ‘and’, etc. You can find more information from <a href="https://openreview.net/pdf?id=SyK00v5xx" target="_blank" rel="noopener noreferrer">this paper</a>. 因为这个的输入直接是句子，没有经过分词的处理，所以不免有 but and 这类的词汇出现。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/><span class="line">12</span><br/><span class="line">13</span><br/><span class="line">14</span><br/><span class="line">15</span><br/><span class="line">16</span><br/><span class="line">17</span><br/><span class="line">18</span><br/><span class="line">19</span><br/><span class="line">20</span><br/><span class="line">21</span><br/><span class="line">22</span><br/><span class="line">23</span><br/><span class="line">24</span><br/><span class="line">25</span><br/><span class="line">26</span><br/><span class="line">27</span><br/><span class="line">28</span><br/><span class="line">29</span><br/><span class="line">30</span><br/><span class="line">31</span><br/><span class="line">32</span><br/><span class="line">33</span><br/><span class="line">34</span><br/><span class="line">35</span><br/><span class="line">36</span><br/><span class="line">37</span><br/></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">remove_first_principal_component</span><span class="params">(X)</span>:</span></span><br/><span class="line">    svd = TruncatedSVD(n_components=<span class="number">1</span>, n_iter=<span class="number">7</span>, random_state=<span class="number">0</span>)</span><br/><span class="line">    svd.fit(X)</span><br/><span class="line">    pc = svd.components_</span><br/><span class="line">    XX = X - X.dot(pc.transpose()) * pc</span><br/><span class="line">    <span class="keyword">return</span> XX</span><br/><span class="line"></span><br/><span class="line"></span><br/><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_sif_benchmark</span><span class="params">(sentences1, sentences2, model, freqs={}, use_stoplist=False, a=<span class="number">0.001</span>)</span>:</span></span><br/><span class="line">    total_freq = sum(freqs.values())</span><br/><span class="line"></span><br/><span class="line">    embeddings = []</span><br/><span class="line"></span><br/><span class="line">    </span><br/><span class="line">    <span class="comment"># common component analysis.</span></span><br/><span class="line">    <span class="keyword">for</span> (sent1, sent2) <span class="keyword">in</span> zip(sentences1, sentences2):</span><br/><span class="line">        tokens1 = sent1.tokens_without_stop <span class="keyword">if</span> use_stoplist <span class="keyword">else</span> sent1.tokens</span><br/><span class="line">        tokens2 = sent2.tokens_without_stop <span class="keyword">if</span> use_stoplist <span class="keyword">else</span> sent2.tokens</span><br/><span class="line"></span><br/><span class="line">        tokens1 = [token <span class="keyword">for</span> token <span class="keyword">in</span> tokens1 <span class="keyword">if</span> token <span class="keyword">in</span> model]</span><br/><span class="line">        tokens2 = [token <span class="keyword">for</span> token <span class="keyword">in</span> tokens2 <span class="keyword">if</span> token <span class="keyword">in</span> model]</span><br/><span class="line"></span><br/><span class="line">        weights1 = [a / (a + freqs.get(token, <span class="number">0</span>) / total_freq) <span class="keyword">for</span> token <span class="keyword">in</span> tokens1]</span><br/><span class="line">        weights2 = [a / (a + freqs.get(token, <span class="number">0</span>) / total_freq) <span class="keyword">for</span> token <span class="keyword">in</span> tokens2]</span><br/><span class="line"></span><br/><span class="line">        embedding1 = np.average([model[token] <span class="keyword">for</span> token <span class="keyword">in</span> tokens1], axis=<span class="number">0</span>, weights=weights1)</span><br/><span class="line">        embedding2 = np.average([model[token] <span class="keyword">for</span> token <span class="keyword">in</span> tokens2], axis=<span class="number">0</span>, weights=weights2)</span><br/><span class="line"></span><br/><span class="line">        embeddings.append(embedding1)</span><br/><span class="line">        embeddings.append(embedding2)</span><br/><span class="line"></span><br/><span class="line">    embeddings = remove_first_principal_component(np.array(embeddings))</span><br/><span class="line">    sims = [cosine_similarity(embeddings[idx * <span class="number">2</span>].reshape(<span class="number">1</span>, <span class="number">-1</span>),</span><br/><span class="line">                              embeddings[idx * <span class="number">2</span> + <span class="number">1</span>].reshape(<span class="number">1</span>, <span class="number">-1</span>))[<span class="number">0</span>][<span class="number">0</span>]</span><br/><span class="line">            <span class="keyword">for</span> idx <span class="keyword">in</span> range(int(len(embeddings) / <span class="number">2</span>))]</span><br/><span class="line"></span><br/><span class="line">    <span class="keyword">return</span> sims</span><br/></pre></td></tr></tbody></table></figure>
<h3 id="Google-Sentence-Encoder"><a href="#Google-Sentence-Encoder" class="headerlink" title="Google Sentence Encoder"></a>Google Sentence Encoder</h3><p><a href="https://github.com/facebookresearch/InferSent" target="_blank" rel="noopener noreferrer">InferSent</a> is a pre-trained encoder that produces sentence embedding, which opensourced by Facebook. <a href="https://tfhub.dev/google/universal-sentence-encoder/1" target="_blank" rel="noopener noreferrer">The Google Sentence Encoder</a> is Google’s answer to Facebook’s InferSent. In contrast to InferSent, the Google Sentence Encoder was trained on a combination of unsupervised data and supervised data (SNLI corpus), which tends to give better results.</p>
<p>The codes can be used in <a href="https://colab.research.google.com/notebooks/welcome.ipynb#recent=true" target="_blank" rel="noopener noreferrer">Google Jupyter Notebook</a></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/><span class="line">12</span><br/><span class="line">13</span><br/><span class="line">14</span><br/><span class="line">15</span><br/><span class="line">16</span><br/><span class="line">17</span><br/><span class="line">18</span><br/><span class="line">19</span><br/><span class="line">20</span><br/><span class="line">21</span><br/><span class="line">22</span><br/><span class="line">23</span><br/><span class="line">24</span><br/><span class="line">25</span><br/></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow_hub <span class="keyword">as</span> hub</span><br/><span class="line"></span><br/><span class="line">tf.logging.set_verbosity(tf.logging.ERROR)</span><br/><span class="line">embed = hub.Module(<span class="string">&#34;https://tfhub.dev/google/universal-sentence-encoder/1&#34;</span>)</span><br/><span class="line"></span><br/><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_gse_benchmark</span><span class="params">(sentences1, sentences2)</span>:</span></span><br/><span class="line">    sts_input1 = tf.placeholder(tf.string, shape=(<span class="keyword">None</span>))</span><br/><span class="line">    sts_input2 = tf.placeholder(tf.string, shape=(<span class="keyword">None</span>))</span><br/><span class="line"></span><br/><span class="line">    sts_encode1 = tf.nn.l2_normalize(embed(sts_input1))</span><br/><span class="line">    sts_encode2 = tf.nn.l2_normalize(embed(sts_input2))</span><br/><span class="line"></span><br/><span class="line">    sim_scores = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=<span class="number">1</span>)</span><br/><span class="line"></span><br/><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br/><span class="line">        session.run(tf.global_variables_initializer())</span><br/><span class="line">        session.run(tf.tables_initializer())</span><br/><span class="line"></span><br/><span class="line">        [gse_sims] = session.run(</span><br/><span class="line">            [sim_scores],</span><br/><span class="line">            feed_dict={</span><br/><span class="line">                sts_input1: [sent1.raw <span class="keyword">for</span> sent1 <span class="keyword">in</span> sentences1],</span><br/><span class="line">                sts_input2: [sent2.raw <span class="keyword">for</span> sent2 <span class="keyword">in</span> sentences2]</span><br/><span class="line">            })</span><br/><span class="line">    <span class="keyword">return</span> gse_sims</span><br/></pre></td></tr></tbody></table></figure>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/><span class="line">12</span><br/><span class="line">13</span><br/><span class="line">14</span><br/></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_experiment</span><span class="params">(df, benchmarks)</span>:</span></span><br/><span class="line">    sentences1 = [Sentence(s) <span class="keyword">for</span> s <span class="keyword">in</span> df[<span class="string">&#39;sent_1&#39;</span>]]</span><br/><span class="line">    sentences2 = [Sentence(s) <span class="keyword">for</span> s <span class="keyword">in</span> df[<span class="string">&#39;sent_2&#39;</span>]]</span><br/><span class="line"></span><br/><span class="line">    pearson_cors, spearman_cors = [], []</span><br/><span class="line">    <span class="keyword">for</span> label, method <span class="keyword">in</span> benchmarks:</span><br/><span class="line">        sims = method(sentences1, sentences2)</span><br/><span class="line">        pearson_correlation = scipy.stats.pearsonr(sims, df[<span class="string">&#39;sim&#39;</span>])[<span class="number">0</span>]</span><br/><span class="line">        print(label, pearson_correlation)</span><br/><span class="line">        pearson_cors.append(pearson_correlation)</span><br/><span class="line">        spearman_correlation = scipy.stats.spearmanr(sims, df[<span class="string">&#39;sim&#39;</span>])[<span class="number">0</span>]</span><br/><span class="line">        spearman_cors.append(spearman_correlation)</span><br/><span class="line"></span><br/><span class="line">    <span class="keyword">return</span> pearson_cors, spearman_cors</span><br/></pre></td></tr></tbody></table></figure>
<p>Helper function:</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> functools <span class="keyword">as</span> ft</span><br/><span class="line"></span><br/><span class="line">benchmarks = [</span><br/><span class="line">    (<span class="string">&#34;AVG-GLOVE&#34;</span>, ft.partial(run_avg_benchmark, model=glove, use_stoplist=<span class="keyword">False</span>)),</span><br/><span class="line">    (<span class="string">&#34;AVG-GLOVE-STOP&#34;</span>, ft.partial(run_avg_benchmark, model=glove, use_stoplist=<span class="keyword">True</span>)),</span><br/><span class="line">    (<span class="string">&#34;AVG-GLOVE-TFIDF&#34;</span>, ft.partial(run_avg_benchmark, model=glove, use_stoplist=<span class="keyword">False</span>, doc_freqs=doc_frequencies)),</span><br/><span class="line">    (<span class="string">&#34;AVG-GLOVE-TFIDF-STOP&#34;</span>, ft.partial(run_avg_benchmark, model=glove, use_stoplist=<span class="keyword">True</span>, doc_freqs=doc_frequencies)),</span><br/><span class="line">    (<span class="string">&#34;SIF-W2V&#34;</span>, ft.partial(run_sif_benchmark, freqs=frequencies, model=word2vec, use_stoplist=<span class="keyword">False</span>)),</span><br/><span class="line">    (<span class="string">&#34;SIF-GLOVE&#34;</span>, ft.partial(run_sif_benchmark, freqs=frequencies, model=glove, use_stoplist=<span class="keyword">False</span>)),</span><br/><span class="line"></span><br/><span class="line">]</span><br/></pre></td></tr></tbody></table></figure>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br/><span class="line">plt.rcParams[<span class="string">&#39;figure.figsize&#39;</span>] = (<span class="number">20</span>,<span class="number">13</span>)</span><br/><span class="line">spearman[[<span class="string">&#39;AVG-GLOVE&#39;</span>, <span class="string">&#39;AVG-GLOVE-STOP&#39;</span>,<span class="string">&#39;AVG-GLOVE-TFIDF&#39;</span>, <span class="string">&#39;AVG-GLOVE-TFIDF-STOP&#39;</span>,<span class="string">&#39;GSE&#39;</span>]].plot(kind=<span class="string">&#34;bar&#34;</span>).legend(loc=<span class="string">&#34;lower left&#34;</span>)</span><br/></pre></td></tr></tbody></table></figure>
<p><strong>Take Off</strong></p>
<ul>
<li>Smooth Inverse Frequency methods are better than baseline, no matter with word2vec or Glove embeddings.</li>
<li>Google Sentence Encoder has the similar performance as Smooth Inverse Frequency.</li>
<li>Using tf-idf weights does not help and using a stoplist looks like a reasonable choice.</li>
</ul>
<p>Pearson Correlation<br/><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1swpqj9d0j20ps0h5t8z.jpg" alt=""/><br/>Spearman Correlation<br/><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1swpqj9d0j20ps0h5t8z.jpg" alt=""/></p>
<p>Full codes can be found in <a href="https://github.com/jijeng/sentence-similarity" target="_blank" rel="noopener noreferrer">here</a>.</p>
<h2 id="复习笔记"><a href="#复习笔记" class="headerlink" title="复习笔记"></a>复习笔记</h2><ol>
<li>TF-IDF 和 SIF三者的差别</li>
</ol>
<p>SIF的计算公式：<br/>$$<br/>operatorname { SIF } ( w ) = frac { a } { ( a + p ( w ) )}<br/>$$<br/>$a$ 是超参数，一般设置为0.001，保证…;  $p(w)$ 是word 在预料中出现的频数。</p>
<p>TF 的计算公式：</p>
<p>$$ 词频(TF) = 某个词在文章中出现的次数( 频数) $$</p>
<p>可以进一步标准化（减少文章长度的影响）</p>
<p>$$ 词频( TF) = frac{某次在文中出现的次数}{文章的总词语数} $$</p>
<p>$$ 逆文档频率 (IDF) = log(frac{语料中的文档总数}{ 包含该词的文档数 +1}) $$ </p>