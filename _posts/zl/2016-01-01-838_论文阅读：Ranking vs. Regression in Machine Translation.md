---
layout: post
title: 论文阅读：Ranking vs. Regression in Machine Translation Evaluation 
tags: [lua文章]
categories: [topic]
---
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>




<p>来源：WMT Metrics Shared Task 2008</p>

<p>链接：<a href="https://dl.acm.org/citation.cfm?doid=1626394.1626425">Abstract</a> or <a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=88FB762456FCA0B2C349C3DA3AE7B9CF?doi=10.1.1.158.9744&amp;rep=rep1&amp;type=pdf">PDF</a></p>

<p>作者：Kevin Duh</p>

<p>单位：UWashington</p>

<p>阅读基础：本文前部为读者普及了Metrics的相关知识，读之即可。另外模型中用到的RankSVM<sup id="fnref:2"><a href="#fn:2" class="footnote">1</a></sup>可以看一下（不看也行，读这篇论文关注的是思想）。</p>



<p>WMT历年来的一个重要任务就是为翻译结果设计评价指标，即WMT Metrics Shared Task. 
metrics是给定译文和参考翻译，评价参考翻译的质量。</p>

<p>与之不同的是，quality estimation（QE）是在不给出参考译文的情况下评估翻译质量。QE只依赖原文和翻译模型，和metrics的数据集是完全不一样的。
QE对应的比赛是WMT Quality Estimation Shared Task.</p>

<h1 id="metrics的应用细节">metrics的应用细节</h1>

<p>Metrics用于在不同翻译系统之间建立比较，作用包括指导单个系统调参，比较系统之间好坏，比较系统的改进（定目标，算进度）等等。</p>

<p>Metrics分为系统级别和句子级别，前者对每一个系统给出一个评价，后者对每一个句子原文及翻译给出一个评价。</p>

<p>Metrics其实是很主观的，所以评价一个metrics的好坏就是看这个指标判断与人类判断的对应程度。
Metrics的一些评价指标通过比较metrics的打分和人类打分的相符程度来评价metrics.</p>

<p>可以参考<sup id="fnref:3"><a href="#fn:3" class="footnote">2</a></sup>入门，里面给出了：</p>

<ul>
  <li>Metrics是什么，为什么要做</li>
  <li>几个非常基础的metrics的baseline和评价指标</li>
  <li>没有给出数据集，但建议看看</li>
</ul>

<h1 id="score-based-metrics-和-ranking-based-metrics">Score-based metrics 和 Ranking-based metrics</h1>

<p>Score-based metrics 为每一句译文给出确切的分数，比如最著名的BLEU就是score-based的。
Ranking-based metrics 对同一原句的不同译文按质量进行排序。
两者的主要区别在于，Score-based metrics 给出的标记是绝对的，Ranking-based metrics 给出的标记是相对的。</p>

<h1 id="ranking-based-metrics-的好处">Ranking-based metrics 的好处</h1>

<p>一句话：和人的判断比较相符。</p>
<ul>
  <li>因为是相对标注，所以人更容易给出精确的标注，这样IAA也会比较高。</li>
  <li>metrics的目的就是比较不同翻译系统的优劣，所以如果可以的话直接给出比较结果就行了，没必要先打绝对评分再比较。</li>
</ul>

<p>虽然 Ranking-based metrics 不能定量刻画系统之间的差异，但是大部分情况下也够用了。</p>

<h1 id="ranking-based-metrics-的可行性">Ranking-based metrics 的可行性</h1>

<p>如何实现一个 Ranking-based metrics 系统？</p>

<p>考虑metrics语料库的格式，将问题形式化：一共$T$个句子，$r_t$是第$t$个句子的参考译文$(1leq tleq T)$，而$o_t^{(n)}$是对第$t$个句子的第$n$个翻译$(1leq nleq N)$。Score-based metrics的标签$y_t = [y_t^{(n)}]_N$是一个$N$维向量，每个维度表示对应译文的得分；而Ranking-based metrics的标签完全一样，但是只比较标签值的大小。</p>

<p><em>（上面描述的形式化过程和原文稍有出入，此处做了简化，但不影响理解全文。）</em></p>

<p>常规的score-based metrics通过$r_t$和$o_t^{(n)}$形成译文的特征向量$x_t^{(n)}$，配合标签$y_t$训练一个回归算法预测打分；而ranking-based metrics也一样通过回归预测得分，然而只取预测结果的排序。</p>



<h1 id="实验">实验</h1>

<p>score-based metrics使用RegressionSVM，ranking-based metrics使用RankSVM/RankBoost</p>

<p>提取的特征多是一些基于统计的metrics，所以实验内容也没啥可看的了，无非展示效果，调参，选择特征云云。</p>

<h1 id="相关数据集">相关数据集</h1>

<p>参考每年最新的WMT任务。</p>

<h1 id="建议阅读">建议阅读</h1>

<p>使用Ranking-based数据集训练metrics模型的思想最早的工作是<sup id="fnref:1"><a href="#fn:1" class="footnote">3</a></sup>，可以看看。</p>

<p>每年最新的WMT建议参考。</p>

<h1 id="参考">参考</h1>

<div class="footnotes">
  <ol>
    <li id="fn:2">
      <p><strong>Joachims 2002 KDD</strong> T. Joachims. 2002. Optimizing search engines using clickthrough data. In KDD <a href="#fnref:2" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:3">
      <p><strong>Bojar 2016 WMT Metrics</strong> <a href="http://www.statmt.org/wmt16/slides/wmt16-metrics.pdf">WMT Metrics 2016 总结</a> <a href="#fnref:3" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:1">
      <p><strong>Ye 2007 WMT Metrics</strong> Y. Ye, M. Zhou, and C.-Y. Lin. 2007. Sentence level machine translation evaluation as a ranking problem. In ACL2007 Wksp on Statistical Machine Translation. <a href="#fnref:1" class="reversefootnote">↩</a></p>
    </li>
  </ol>
</div>